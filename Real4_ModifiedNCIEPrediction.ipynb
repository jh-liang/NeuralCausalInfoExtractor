{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell/anaconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "etth1data = pd.read_csv('dataset/ETT-small/ETTh1.csv')\n",
    "etth1data = etth1data.drop('date', axis=1)\n",
    "target = etth1data.pop('OT')\n",
    "etth1data.insert(0, 'OT', target)\n",
    "data1 = etth1data\n",
    "# pcmci_result = np.array([8, -1, 10, -1, 11,  4, 16, 20, 14, -1, 19, 13, -1,  9, 13,  7, 10,\n",
    "#                             -1, 17, 17,  3, 16,  0, 16, 13])\n",
    "pcmci_result = np.array([1, 1, 3, 1, 3, 2, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrodata = pd.read_csv('dataset/Metro_Interstate_Traffic_Volume.csv').drop(['weather_description', 'date_time'], axis=1)\n",
    "target = metrodata.pop('traffic_volume')\n",
    "metrodata.insert(0, 'OT', target)\n",
    "metrodata.loc[metrodata['holiday'] == 'None', 'holiday'] = 0\n",
    "metrodata.loc[metrodata['holiday'] != 0, 'holiday'] = 1\n",
    "metrodata['holiday'] = metrodata['holiday'].astype('int64')\n",
    "metrodata['weather_main'] = metrodata['weather_main'].map({'Clouds':0. , 'Clear':1., 'Rain':2., 'Drizzle':3., 'Mist':4., 'Haze':5., 'Fog':6.,\n",
    "                                                            'Thunderstorm':7., 'Snow':8., 'Squall':9., 'Smoke':10.})\n",
    "data1 = metrodata\n",
    "pcmci_result = np.array([1, 1, 1, 1, 1, 1, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchangedata = pd.read_csv('dataset/exchange_rate/exchange_rate.csv')\n",
    "exchangedata = exchangedata.drop('date', axis=1)\n",
    "target = exchangedata.pop('OT')\n",
    "exchangedata.insert(0, 'OT', target)\n",
    "data1 = exchangedata\n",
    "pcmci_result = np.array([1, -1, 3, 5, 3, -1, 1, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmdata = pd.read_csv('dataset/beijing_air.csv')\n",
    "pmdata['date'] = pd.to_datetime(pmdata['date'])\n",
    "pmdata['year'] = pmdata['date'].apply(lambda x:x.year)\n",
    "pmdata['month'] = pmdata['date'].apply(lambda x:x.month)\n",
    "pmdata['day'] = pmdata['date'].apply(lambda x:x.day)\n",
    "pmdata['hour'] = pmdata['date'].apply(lambda x:x.hour)\n",
    "pmdata = pmdata.drop(['date', 'wnd_dir'], axis=1)\n",
    "pmdata.dtypes\n",
    "\n",
    "data1 = pmdata\n",
    "pcmci_result = np.array([1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = data1.iloc[:, pcmci_result > -1]\n",
    "sorted_arrows = pcmci_result[pcmci_result > -1].astype(int)\n",
    "\n",
    "tau_max = 20\n",
    "ar_range = 10\n",
    "\n",
    "lagged_data = pd.DataFrame(sorted_data.iloc[tau_max:, 0])\n",
    "for i in range(1, ar_range):\n",
    "    lagged_data['OT' + '_' + str(i)] = np.array(sorted_data.iloc[tau_max-i:-i, 0])\n",
    "for i in range(sorted_data.shape[1]):\n",
    "    if sorted_arrows[i] == 0:\n",
    "        lagged_data[sorted_data.columns[i] + '_' + str(sorted_arrows[i])] = np.array(sorted_data.iloc[tau_max:, i])\n",
    "        continue\n",
    "    lagged_data[sorted_data.columns[i] + '_' + str(sorted_arrows[i])] = np.array(sorted_data.iloc[tau_max - sorted_arrows[i]:-sorted_arrows[i], i])\n",
    "\n",
    "minmaxgap =  lagged_data.iloc[:, 0].max() - lagged_data.iloc[:, 0].min()\n",
    "lagged_data = (lagged_data - lagged_data.min()) / ( lagged_data.max() - lagged_data.min())\n",
    "\n",
    "data2 = lagged_data.copy()\n",
    "data = np.array(data2)\n",
    "data.shape\n",
    "\n",
    "train_proportion = 0.6\n",
    "valid_proportion = 0.2\n",
    "test_proportion = 0.2\n",
    "\n",
    "train_size = int(data.shape[0] * train_proportion)\n",
    "valid_size = int(data.shape[0] * valid_proportion)\n",
    "\n",
    "train_data = data[:train_size]\n",
    "valid_data = data[train_size:train_size+valid_size]\n",
    "test_data = data[train_size+valid_size:]\n",
    "\n",
    "batch_size = 20\n",
    "window_size = 400\n",
    "\n",
    "z_size = data.shape[1]\n",
    "\n",
    "\n",
    "Z = np.load('results/new_exchange_rate_z.npy')\n",
    "\n",
    "train_Z = Z[:train_size]\n",
    "valid_Z = Z[train_size:train_size+valid_size]\n",
    "test_Z = Z[train_size+valid_size:]\n",
    "\n",
    "###\n",
    "valid_Z = torch.autograd.Variable(torch.FloatTensor(valid_Z)).cuda()\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_1199165/2110840739.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "100%|██████████| 50000/50000 [15:30<00:00, 53.71it/s]\n",
      "100%|██████████| 50000/50000 [16:07<00:00, 51.68it/s]\n",
      "100%|██████████| 50000/50000 [16:02<00:00, 51.92it/s]\n",
      "100%|██████████| 50000/50000 [16:06<00:00, 51.74it/s]\n",
      "100%|██████████| 50000/50000 [16:05<00:00, 51.81it/s]\n",
      "100%|██████████| 50000/50000 [16:10<00:00, 51.50it/s]\n",
      "100%|██████████| 50000/50000 [16:04<00:00, 51.83it/s]\n",
      "100%|██████████| 50000/50000 [16:08<00:00, 51.64it/s]\n",
      "100%|██████████| 50000/50000 [16:07<00:00, 51.70it/s]\n",
      "100%|██████████| 50000/50000 [16:05<00:00, 51.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005784976986138886\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden_size = 32, num_layers = 1, output_size = 2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,     # rnn hidden unit\n",
    "            num_layers = num_layers,       # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        # print(r_out.shape)\n",
    "        # outs = []    # save all predictions\n",
    "        # for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "        #     outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        # return torch.stack(outs, dim=1), h_state\n",
    "\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "\n",
    "def train_reg(data, Z, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    # X_lagged = np.concatenate((np.ones(shape=(1,1), dtype=np.float)*data[0,0], np.expand_dims((data[0:-1:, 0]), 0)), 1)\n",
    "    # Y = np.concatenate((X_lagged.T, data[:, 1:]), 1)\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "        Z_ = Z[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(torch.cat((Y_, Z_), axis = 1), h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(torch.cat((valid_Y, valid_Z), axis = 1), h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def test(data, Z, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "    \n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(torch.cat((Y, Z), axis = 1), h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_optim = optim.Adam(regr_net.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred = train_reg(train_data, train_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred = train_reg(valid_data, valid_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "\n",
    "    regr_trained = regr_net\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred = test(test_data, test_Z, regr_net)\n",
    "    MSEloss = np.mean(np.square(test_data[10:,0] - X_pred[10:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_1199165/1802000487.py:39: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [16:02<00:00, 51.96it/s]\n",
      "100%|██████████| 50000/50000 [16:08<00:00, 51.61it/s]\n",
      "100%|██████████| 50000/50000 [16:12<00:00, 51.43it/s]\n",
      "100%|██████████| 50000/50000 [16:11<00:00, 51.49it/s]\n",
      "100%|██████████| 50000/50000 [16:04<00:00, 51.83it/s]\n",
      "100%|██████████| 50000/50000 [16:03<00:00, 51.91it/s]\n",
      "100%|██████████| 50000/50000 [16:03<00:00, 51.89it/s]\n",
      "100%|██████████| 50000/50000 [16:03<00:00, 51.88it/s]\n",
      "100%|██████████| 50000/50000 [16:05<00:00, 51.81it/s]\n",
      "100%|██████████| 50000/50000 [16:09<00:00, 51.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005867390404610108\n"
     ]
    }
   ],
   "source": [
    "def test_without_Z(data, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(Y, h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "def train_reg_withoutZ(data, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(Y_, h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(valid_Y, h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z_optim = optim.Adam(regr_net_without_Z.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred_without_Z = train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred_without_Z = train_reg_withoutZ(valid_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "\n",
    "    regr_without_Z_trained = regr_net_without_Z\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred_without_Z = test_without_Z(test_data, regr_net_without_Z)\n",
    "    MSEloss = np.mean(np.square(test_data[:,0] - X_pred_without_Z[:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_1199165/3099682845.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "100%|██████████| 50000/50000 [21:44<00:00, 38.33it/s]\n",
      "100%|██████████| 50000/50000 [21:36<00:00, 38.58it/s]\n",
      "100%|██████████| 50000/50000 [21:31<00:00, 38.72it/s]\n",
      "100%|██████████| 50000/50000 [21:35<00:00, 38.60it/s]\n",
      "100%|██████████| 50000/50000 [21:34<00:00, 38.62it/s]\n",
      "100%|██████████| 50000/50000 [21:35<00:00, 38.59it/s]\n",
      "100%|██████████| 50000/50000 [21:33<00:00, 38.66it/s]\n",
      "100%|██████████| 50000/50000 [21:35<00:00, 38.60it/s]\n",
      "100%|██████████| 50000/50000 [21:34<00:00, 38.62it/s]\n",
      "100%|██████████| 50000/50000 [21:31<00:00, 38.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005699555803152768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden_size = 32, num_layers = 1, output_size = 2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,     # rnn hidden unit\n",
    "            num_layers = num_layers,       # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        # print(r_out.shape)\n",
    "        # outs = []    # save all predictions\n",
    "        # for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "        #     outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        # return torch.stack(outs, dim=1), h_state\n",
    "\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "\n",
    "def train_reg(data, Z, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    # X_lagged = np.concatenate((np.ones(shape=(1,1), dtype=np.float)*data[0,0], np.expand_dims((data[0:-1:, 0]), 0)), 1)\n",
    "    # Y = np.concatenate((X_lagged.T, data[:, 1:]), 1)\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "        Z_ = Z[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(torch.cat((Y_, Z_), axis = 1), h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(torch.cat((valid_Y, valid_Z), axis = 1), h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def test(data, Z, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "    \n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(torch.cat((Y, Z), axis = 1), h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_optim = optim.Adam(regr_net.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred = train_reg(train_data, train_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred = train_reg(valid_data, valid_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "\n",
    "    regr_trained = regr_net\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred = test(test_data, test_Z, regr_net)\n",
    "    MSEloss = np.mean(np.square(test_data[10:,0] - X_pred[10:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1199165/105458899.py:39: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "100%|██████████| 50000/50000 [23:55<00:00, 34.82it/s]\n",
      "100%|██████████| 50000/50000 [21:50<00:00, 38.15it/s]\n",
      "100%|██████████| 50000/50000 [21:33<00:00, 38.65it/s]\n",
      "100%|██████████| 50000/50000 [21:32<00:00, 38.69it/s]\n",
      "100%|██████████| 50000/50000 [21:33<00:00, 38.67it/s]\n",
      "100%|██████████| 50000/50000 [21:36<00:00, 38.58it/s]\n",
      "100%|██████████| 50000/50000 [21:29<00:00, 38.76it/s]\n",
      "100%|██████████| 50000/50000 [21:34<00:00, 38.64it/s]\n",
      "100%|██████████| 50000/50000 [21:28<00:00, 38.80it/s]\n",
      "100%|██████████| 50000/50000 [22:08<00:00, 37.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005873271280338906\n"
     ]
    }
   ],
   "source": [
    "def test_without_Z(data, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(Y, h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "def train_reg_withoutZ(data, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "        \n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(Y_, h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(valid_Y, h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z_optim = optim.Adam(regr_net_without_Z.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred_without_Z = train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred_without_Z = train_reg_withoutZ(valid_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "\n",
    "    regr_without_Z_trained = regr_net_without_Z\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred_without_Z = test_without_Z(test_data, regr_net_without_Z)\n",
    "    MSEloss = np.mean(np.square(test_data[:,0] - X_pred_without_Z[:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "etth2data = pd.read_csv('dataset/ETT-small/ETTh2.csv')\n",
    "etth2data = etth2data.drop('date', axis=1)\n",
    "target = etth2data.pop('OT')\n",
    "etth2data.insert(0, 'OT', target)\n",
    "data1 = etth2data\n",
    "pcmci_result = np.array([1, 1, 1, 1, 1, 1, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmdata = pd.read_csv('dataset/beijing_air.csv')\n",
    "pmdata['date'] = pd.to_datetime(pmdata['date'])\n",
    "pmdata['year'] = pmdata['date'].apply(lambda x:x.year)\n",
    "pmdata['month'] = pmdata['date'].apply(lambda x:x.month)\n",
    "pmdata['day'] = pmdata['date'].apply(lambda x:x.day)\n",
    "pmdata['hour'] = pmdata['date'].apply(lambda x:x.hour)\n",
    "pmdata = pmdata.drop(['date', 'wnd_dir'], axis=1)\n",
    "pmdata.dtypes\n",
    "\n",
    "data1 = pmdata\n",
    "pcmci_result = np.array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = data1.iloc[:, pcmci_result > -1]\n",
    "sorted_arrows = pcmci_result[pcmci_result > -1].astype(int)\n",
    "\n",
    "tau_max = 20\n",
    "ar_range = 10\n",
    "\n",
    "lagged_data = pd.DataFrame(sorted_data.iloc[tau_max:, 0])\n",
    "for i in range(1, ar_range):\n",
    "    lagged_data['Target' + '_' + str(i)] = np.array(sorted_data.iloc[tau_max-i:-i, 0])\n",
    "for i in range(sorted_data.shape[1]):\n",
    "    if sorted_arrows[i] == 0:\n",
    "        lagged_data[sorted_data.columns[i] + '_' + str(sorted_arrows[i])] = np.array(sorted_data.iloc[tau_max:, i])\n",
    "        continue\n",
    "    lagged_data[sorted_data.columns[i] + '_' + str(sorted_arrows[i])] = np.array(sorted_data.iloc[tau_max - sorted_arrows[i]:-sorted_arrows[i], i])\n",
    "\n",
    "minmaxgap =  lagged_data.iloc[:, 0].max() - lagged_data.iloc[:, 0].min()\n",
    "lagged_data = (lagged_data - lagged_data.min()) / ( lagged_data.max() - lagged_data.min())\n",
    "\n",
    "data2 = lagged_data.copy()\n",
    "data = np.array(data2)\n",
    "data.shape\n",
    "\n",
    "train_proportion = 0.6\n",
    "valid_proportion = 0.2\n",
    "test_proportion = 0.2\n",
    "\n",
    "train_size = int(data.shape[0] * train_proportion)\n",
    "valid_size = int(data.shape[0] * valid_proportion)\n",
    "\n",
    "train_data = data[:train_size]\n",
    "valid_data = data[train_size:train_size+valid_size]\n",
    "test_data = data[train_size+valid_size:]\n",
    "\n",
    "batch_size = 20\n",
    "window_size = 400\n",
    "\n",
    "z_size = data.shape[1]\n",
    "\n",
    "\n",
    "Z = np.load('results/pm_z.npy')\n",
    "\n",
    "train_Z = Z[:train_size]\n",
    "valid_Z = Z[train_size:train_size+valid_size]\n",
    "test_Z = Z[train_size+valid_size:]\n",
    "\n",
    "###\n",
    "valid_Z = torch.autograd.Variable(torch.FloatTensor(valid_Z)).cuda()\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_967354/2681342656.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "100%|██████████| 50000/50000 [16:06<00:00, 51.71it/s]\n",
      "100%|██████████| 50000/50000 [16:01<00:00, 52.02it/s]\n",
      "100%|██████████| 50000/50000 [16:12<00:00, 51.41it/s]\n",
      "100%|██████████| 50000/50000 [15:58<00:00, 52.16it/s]\n",
      "100%|██████████| 50000/50000 [15:59<00:00, 52.11it/s]\n",
      "100%|██████████| 50000/50000 [16:00<00:00, 52.07it/s]\n",
      "100%|██████████| 50000/50000 [16:00<00:00, 52.07it/s]\n",
      "100%|██████████| 50000/50000 [15:57<00:00, 52.24it/s]\n",
      "100%|██████████| 50000/50000 [15:57<00:00, 52.22it/s]\n",
      "100%|██████████| 50000/50000 [15:58<00:00, 52.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005847809841388942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden_size = 32, num_layers = 1, output_size = 2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,     # rnn hidden unit\n",
    "            num_layers = num_layers,       # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        # print(r_out.shape)\n",
    "        # outs = []    # save all predictions\n",
    "        # for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "        #     outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        # return torch.stack(outs, dim=1), h_state\n",
    "\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "\n",
    "def train_reg(data, Z, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    # X_lagged = np.concatenate((np.ones(shape=(1,1), dtype=np.float)*data[0,0], np.expand_dims((data[0:-1:, 0]), 0)), 1)\n",
    "    # Y = np.concatenate((X_lagged.T, data[:, 1:]), 1)\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "        Z_ = Z[pivot:pivot+window_size]\n",
    "    \n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(torch.cat((Y_, Z_), axis = 1), h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(torch.cat((valid_Y, valid_Z), axis = 1), h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def test(data, Z, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "    \n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(torch.cat((Y, Z), axis = 1), h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_optim = optim.Adam(regr_net.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred = train_reg(train_data, train_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred = train_reg(valid_data, valid_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "\n",
    "    regr_trained = regr_net\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred = test(test_data, test_Z, regr_net)\n",
    "    MSEloss = np.mean(np.square(test_data[10:,0] - X_pred[10:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_967354/4262776442.py:39: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "100%|██████████| 50000/50000 [15:56<00:00, 52.25it/s]\n",
      "100%|██████████| 50000/50000 [15:56<00:00, 52.30it/s]\n",
      "100%|██████████| 50000/50000 [16:00<00:00, 52.06it/s]\n",
      "100%|██████████| 50000/50000 [15:57<00:00, 52.24it/s]\n",
      "100%|██████████| 50000/50000 [15:56<00:00, 52.27it/s]\n",
      "100%|██████████| 50000/50000 [15:52<00:00, 52.48it/s]\n",
      "100%|██████████| 50000/50000 [15:54<00:00, 52.36it/s]\n",
      "100%|██████████| 50000/50000 [16:01<00:00, 52.00it/s]\n",
      "100%|██████████| 50000/50000 [15:57<00:00, 52.22it/s]\n",
      "100%|██████████| 50000/50000 [15:55<00:00, 52.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000587423369750537\n"
     ]
    }
   ],
   "source": [
    "def test_without_Z(data, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(Y, h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "def train_reg_withoutZ(data, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(Y_, h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(valid_Y, h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z_optim = optim.Adam(regr_net_without_Z.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred_without_Z = train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred_without_Z = train_reg_withoutZ(valid_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "\n",
    "    regr_without_Z_trained = regr_net_without_Z\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred_without_Z = test_without_Z(test_data, regr_net_without_Z)\n",
    "    MSEloss = np.mean(np.square(test_data[:,0] - X_pred_without_Z[:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_967354/3294305005.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "100%|██████████| 50000/50000 [21:16<00:00, 39.18it/s]\n",
      "100%|██████████| 50000/50000 [21:19<00:00, 39.06it/s]\n",
      "100%|██████████| 50000/50000 [21:17<00:00, 39.14it/s]\n",
      "100%|██████████| 50000/50000 [21:18<00:00, 39.10it/s]\n",
      "100%|██████████| 50000/50000 [21:19<00:00, 39.08it/s]\n",
      "100%|██████████| 50000/50000 [21:18<00:00, 39.11it/s]\n",
      "100%|██████████| 50000/50000 [21:21<00:00, 39.00it/s]\n",
      "100%|██████████| 50000/50000 [21:20<00:00, 39.05it/s]\n",
      "100%|██████████| 50000/50000 [21:16<00:00, 39.17it/s]\n",
      "100%|██████████| 50000/50000 [21:17<00:00, 39.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005830097096143605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden_size = 32, num_layers = 1, output_size = 2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,     # rnn hidden unit\n",
    "            num_layers = num_layers,       # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        # print(r_out.shape)\n",
    "        # outs = []    # save all predictions\n",
    "        # for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "        #     outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        # return torch.stack(outs, dim=1), h_state\n",
    "\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "\n",
    "def train_reg(data, Z, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    # X_lagged = np.concatenate((np.ones(shape=(1,1), dtype=np.float)*data[0,0], np.expand_dims((data[0:-1:, 0]), 0)), 1)\n",
    "    # Y = np.concatenate((X_lagged.T, data[:, 1:]), 1)\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "        Z_ = Z[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(torch.cat((Y_, Z_), axis = 1), h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(torch.cat((valid_Y, valid_Z), axis = 1), h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def test(data, Z, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "    \n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(torch.cat((Y, Z), axis = 1), h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_optim = optim.Adam(regr_net.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred = train_reg(train_data, train_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred = train_reg(valid_data, valid_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "\n",
    "    regr_trained = regr_net\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred = test(test_data, test_Z, regr_net)\n",
    "    MSEloss = np.mean(np.square(test_data[10:,0] - X_pred[10:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_967354/1802000487.py:39: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "100%|██████████| 50000/50000 [23:57<00:00, 34.77it/s]\n",
      "100%|██████████| 50000/50000 [23:58<00:00, 34.76it/s]\n",
      "100%|██████████| 50000/50000 [23:57<00:00, 34.78it/s]\n",
      "100%|██████████| 50000/50000 [23:54<00:00, 34.86it/s]\n",
      "100%|██████████| 50000/50000 [23:52<00:00, 34.91it/s]\n",
      "100%|██████████| 50000/50000 [23:31<00:00, 35.42it/s]\n",
      "100%|██████████| 50000/50000 [21:26<00:00, 38.88it/s]\n",
      "100%|██████████| 50000/50000 [21:47<00:00, 38.24it/s]\n",
      "100%|██████████| 50000/50000 [23:12<00:00, 35.91it/s]\n",
      "100%|██████████| 50000/50000 [23:06<00:00, 36.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0005857808317175381\n"
     ]
    }
   ],
   "source": [
    "def test_without_Z(data, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(Y, h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "def train_reg_withoutZ(data, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(Y_, h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(valid_Y, h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z_optim = optim.Adam(regr_net_without_Z.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred_without_Z = train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred_without_Z = train_reg_withoutZ(valid_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "\n",
    "    regr_without_Z_trained = regr_net_without_Z\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred_without_Z = test_without_Z(test_data, regr_net_without_Z)\n",
    "    MSEloss = np.mean(np.square(test_data[:,0] - X_pred_without_Z[:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ettm1data = pd.read_csv('dataset/ETT-small/ETTm1.csv')\n",
    "ettm1data = ettm1data.drop('date', axis=1)\n",
    "target = ettm1data.pop('OT')\n",
    "ettm1data.insert(0, 'OT', target)\n",
    "data1 = ettm1data\n",
    "pcmci_result = np.array([1, 1, 1, 1, 1, 1, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = data1.iloc[:, pcmci_result > -1]\n",
    "sorted_arrows = pcmci_result[pcmci_result > -1].astype(int)\n",
    "\n",
    "tau_max = 20\n",
    "ar_range = 10\n",
    "\n",
    "lagged_data = pd.DataFrame(sorted_data.iloc[tau_max:, 0])\n",
    "for i in range(1, ar_range):\n",
    "    lagged_data['Target' + '_' + str(i)] = np.array(sorted_data.iloc[tau_max-i:-i, 0])\n",
    "for i in range(sorted_data.shape[1]):\n",
    "    if sorted_arrows[i] == 0:\n",
    "        lagged_data[sorted_data.columns[i] + '_' + str(sorted_arrows[i])] = np.array(sorted_data.iloc[tau_max:, i])\n",
    "        continue\n",
    "    lagged_data[sorted_data.columns[i] + '_' + str(sorted_arrows[i])] = np.array(sorted_data.iloc[tau_max - sorted_arrows[i]:-sorted_arrows[i], i])\n",
    "\n",
    "minmaxgap =  lagged_data.iloc[:, 0].max() - lagged_data.iloc[:, 0].min()\n",
    "lagged_data = (lagged_data - lagged_data.min()) / ( lagged_data.max() - lagged_data.min())\n",
    "\n",
    "data2 = lagged_data.copy()\n",
    "data = np.array(data2)\n",
    "data.shape\n",
    "\n",
    "train_proportion = 0.6\n",
    "valid_proportion = 0.2\n",
    "test_proportion = 0.2\n",
    "\n",
    "train_size = int(data.shape[0] * train_proportion)\n",
    "valid_size = int(data.shape[0] * valid_proportion)\n",
    "\n",
    "train_data = data[:train_size]\n",
    "valid_data = data[train_size:train_size+valid_size]\n",
    "test_data = data[train_size+valid_size:]\n",
    "\n",
    "batch_size = 20\n",
    "window_size = 400\n",
    "\n",
    "z_size = data.shape[1]\n",
    "\n",
    "\n",
    "Z = np.load('results/ettm1_z.npy')\n",
    "\n",
    "train_Z = Z[:train_size]\n",
    "valid_Z = Z[train_size:train_size+valid_size]\n",
    "test_Z = Z[train_size+valid_size:]\n",
    "\n",
    "###\n",
    "valid_Z = torch.autograd.Variable(torch.FloatTensor(valid_Z)).cuda()\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5021/2110840739.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "100%|██████████| 50000/50000 [23:38<00:00, 35.24it/s]\n",
      "100%|██████████| 50000/50000 [23:13<00:00, 35.88it/s]\n",
      "100%|██████████| 50000/50000 [23:37<00:00, 35.27it/s]\n",
      "100%|██████████| 50000/50000 [23:26<00:00, 35.54it/s]\n",
      "100%|██████████| 50000/50000 [23:22<00:00, 35.66it/s]\n",
      "100%|██████████| 50000/50000 [23:22<00:00, 35.65it/s]\n",
      "100%|██████████| 50000/50000 [23:27<00:00, 35.51it/s]\n",
      "100%|██████████| 50000/50000 [23:32<00:00, 35.40it/s]\n",
      "100%|██████████| 50000/50000 [23:34<00:00, 35.35it/s]\n",
      "100%|██████████| 50000/50000 [23:28<00:00, 35.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2374494963350835e-05\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden_size = 32, num_layers = 1, output_size = 2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,     # rnn hidden unit\n",
    "            num_layers = num_layers,       # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        # print(r_out.shape)\n",
    "        # outs = []    # save all predictions\n",
    "        # for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "        #     outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        # return torch.stack(outs, dim=1), h_state\n",
    "\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "\n",
    "def train_reg(data, Z, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    # X_lagged = np.concatenate((np.ones(shape=(1,1), dtype=np.float)*data[0,0], np.expand_dims((data[0:-1:, 0]), 0)), 1)\n",
    "    # Y = np.concatenate((X_lagged.T, data[:, 1:]), 1)\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "        Z_ = Z[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(torch.cat((Y_, Z_), axis = 1), h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(torch.cat((valid_Y, valid_Z), axis = 1), h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def test(data, Z, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "    \n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(torch.cat((Y, Z), axis = 1), h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_optim = optim.Adam(regr_net.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred = train_reg(train_data, train_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred = train_reg(valid_data, valid_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "\n",
    "    regr_trained = regr_net\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred = test(test_data, test_Z, regr_net)\n",
    "    MSEloss = np.mean(np.square(test_data[10:,0] - X_pred[10:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_5021/4262776442.py:39: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "  0%|          | 20/50000 [00:00<26:00, 32.02it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [23:31<00:00, 35.42it/s]\n",
      "100%|██████████| 50000/50000 [23:42<00:00, 35.14it/s]\n",
      "100%|██████████| 50000/50000 [23:40<00:00, 35.21it/s]\n",
      "100%|██████████| 50000/50000 [23:31<00:00, 35.41it/s]\n",
      "100%|██████████| 50000/50000 [23:18<00:00, 35.76it/s]\n",
      "100%|██████████| 50000/50000 [23:19<00:00, 35.73it/s]\n",
      "100%|██████████| 50000/50000 [23:19<00:00, 35.72it/s]\n",
      "100%|██████████| 50000/50000 [23:15<00:00, 35.83it/s]\n",
      "100%|██████████| 50000/50000 [23:15<00:00, 35.83it/s]\n",
      "100%|██████████| 50000/50000 [23:32<00:00, 35.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.350068194598595e-05\n"
     ]
    }
   ],
   "source": [
    "def test_without_Z(data, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(Y, h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "def train_reg_withoutZ(data, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(Y_, h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(valid_Y, h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z_optim = optim.Adam(regr_net_without_Z.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred_without_Z = train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred_without_Z = train_reg_withoutZ(valid_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "\n",
    "    regr_without_Z_trained = regr_net_without_Z\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred_without_Z = test_without_Z(test_data, regr_net_without_Z)\n",
    "    MSEloss = np.mean(np.square(test_data[:,0] - X_pred_without_Z[:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5021/3294305005.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "100%|██████████| 50000/50000 [34:35<00:00, 24.09it/s]\n",
      "100%|██████████| 50000/50000 [34:35<00:00, 24.10it/s]\n",
      "100%|██████████| 50000/50000 [34:31<00:00, 24.14it/s]\n",
      "100%|██████████| 50000/50000 [34:42<00:00, 24.01it/s]\n",
      "100%|██████████| 50000/50000 [34:40<00:00, 24.03it/s]\n",
      "100%|██████████| 50000/50000 [34:38<00:00, 24.05it/s]\n",
      "100%|██████████| 50000/50000 [34:36<00:00, 24.08it/s]\n",
      "100%|██████████| 50000/50000 [34:42<00:00, 24.01it/s]\n",
      "100%|██████████| 50000/50000 [34:36<00:00, 24.08it/s]\n",
      "100%|██████████| 50000/50000 [34:37<00:00, 24.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.121707533779109e-05\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden_size = 32, num_layers = 1, output_size = 2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,     # rnn hidden unit\n",
    "            num_layers = num_layers,       # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        # print(r_out.shape)\n",
    "        # outs = []    # save all predictions\n",
    "        # for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "        #     outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        # return torch.stack(outs, dim=1), h_state\n",
    "\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "\n",
    "def train_reg(data, Z, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    # X_lagged = np.concatenate((np.ones(shape=(1,1), dtype=np.float)*data[0,0], np.expand_dims((data[0:-1:, 0]), 0)), 1)\n",
    "    # Y = np.concatenate((X_lagged.T, data[:, 1:]), 1)\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "        Z_ = Z[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(torch.cat((Y_, Z_), axis = 1), h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(torch.cat((valid_Y, valid_Z), axis = 1), h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def test(data, Z, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "    \n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(torch.cat((Y, Z), axis = 1), h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_optim = optim.Adam(regr_net.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred = train_reg(train_data, train_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred = train_reg(valid_data, valid_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "\n",
    "    regr_trained = regr_net\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred = test(test_data, test_Z, regr_net)\n",
    "    MSEloss = np.mean(np.square(test_data[10:,0] - X_pred[10:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_5021/1802000487.py:39: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "  0%|          | 15/50000 [00:00<33:39, 24.75it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [33:44<00:00, 24.70it/s]\n",
      "100%|██████████| 50000/50000 [33:41<00:00, 24.73it/s]\n",
      "100%|██████████| 50000/50000 [33:43<00:00, 24.71it/s]\n",
      "100%|██████████| 50000/50000 [33:48<00:00, 24.64it/s]\n",
      "100%|██████████| 50000/50000 [33:39<00:00, 24.76it/s]\n",
      "100%|██████████| 50000/50000 [33:52<00:00, 24.60it/s]\n",
      "100%|██████████| 50000/50000 [33:46<00:00, 24.67it/s]\n",
      "100%|██████████| 50000/50000 [33:48<00:00, 24.65it/s]\n",
      "100%|██████████| 50000/50000 [33:40<00:00, 24.74it/s]\n",
      "100%|██████████| 50000/50000 [33:44<00:00, 24.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.565377045861268e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test_without_Z(data, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(Y, h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "def train_reg_withoutZ(data, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(Y_, h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(valid_Y, h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z_optim = optim.Adam(regr_net_without_Z.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred_without_Z = train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred_without_Z = train_reg_withoutZ(valid_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "\n",
    "    regr_without_Z_trained = regr_net_without_Z\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred_without_Z = test_without_Z(test_data, regr_net_without_Z)\n",
    "    MSEloss = np.mean(np.square(test_data[:,0] - X_pred_without_Z[:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ettm2data = pd.read_csv('dataset/ETT-small/ETTm2.csv')\n",
    "ettm2data = ettm2data.drop('date', axis=1)\n",
    "target = ettm2data.pop('OT')\n",
    "ettm2data.insert(0, 'OT', target)\n",
    "data1 = ettm2data\n",
    "pcmci_result = np.array([1, 1, 1, 1, 1, 1, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = data1.iloc[:, pcmci_result > -1]\n",
    "sorted_arrows = pcmci_result[pcmci_result > -1].astype(int)\n",
    "\n",
    "tau_max = 20\n",
    "ar_range = 10\n",
    "\n",
    "lagged_data = pd.DataFrame(sorted_data.iloc[tau_max:, 0])\n",
    "for i in range(1, ar_range):\n",
    "    lagged_data['Target' + '_' + str(i)] = np.array(sorted_data.iloc[tau_max-i:-i, 0])\n",
    "for i in range(sorted_data.shape[1]):\n",
    "    if sorted_arrows[i] == 0:\n",
    "        lagged_data[sorted_data.columns[i] + '_' + str(sorted_arrows[i])] = np.array(sorted_data.iloc[tau_max:, i])\n",
    "        continue\n",
    "    lagged_data[sorted_data.columns[i] + '_' + str(sorted_arrows[i])] = np.array(sorted_data.iloc[tau_max - sorted_arrows[i]:-sorted_arrows[i], i])\n",
    "\n",
    "minmaxgap =  lagged_data.iloc[:, 0].max() - lagged_data.iloc[:, 0].min()\n",
    "lagged_data = (lagged_data - lagged_data.min()) / ( lagged_data.max() - lagged_data.min())\n",
    "\n",
    "data2 = lagged_data.copy()\n",
    "data = np.array(data2)\n",
    "data.shape\n",
    "\n",
    "train_proportion = 0.6\n",
    "valid_proportion = 0.2\n",
    "test_proportion = 0.2\n",
    "\n",
    "train_size = int(data.shape[0] * train_proportion)\n",
    "valid_size = int(data.shape[0] * valid_proportion)\n",
    "\n",
    "train_data = data[:train_size]\n",
    "valid_data = data[train_size:train_size+valid_size]\n",
    "test_data = data[train_size+valid_size:]\n",
    "\n",
    "batch_size = 20\n",
    "window_size = 400\n",
    "\n",
    "z_size = data.shape[1]\n",
    "\n",
    "\n",
    "Z = np.load('results/ettm2_z.npy')\n",
    "\n",
    "train_Z = Z[:train_size]\n",
    "valid_Z = Z[train_size:train_size+valid_size]\n",
    "test_Z = Z[train_size+valid_size:]\n",
    "\n",
    "###\n",
    "valid_Z = torch.autograd.Variable(torch.FloatTensor(valid_Z)).cuda()\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_586517/2110840739.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "100%|██████████| 50000/50000 [24:41<00:00, 33.74it/s]\n",
      "100%|██████████| 50000/50000 [26:31<00:00, 31.41it/s]\n",
      "100%|██████████| 50000/50000 [26:19<00:00, 31.65it/s]\n",
      "100%|██████████| 50000/50000 [26:25<00:00, 31.55it/s]\n",
      "100%|██████████| 50000/50000 [26:25<00:00, 31.53it/s]\n",
      "100%|██████████| 50000/50000 [26:23<00:00, 31.58it/s]\n",
      "100%|██████████| 50000/50000 [26:23<00:00, 31.57it/s]\n",
      "100%|██████████| 50000/50000 [26:28<00:00, 31.48it/s]\n",
      "100%|██████████| 50000/50000 [26:24<00:00, 31.56it/s]\n",
      "100%|██████████| 50000/50000 [26:31<00:00, 31.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0292250798249835e-05\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden_size = 32, num_layers = 1, output_size = 2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,     # rnn hidden unit\n",
    "            num_layers = num_layers,       # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        # print(r_out.shape)\n",
    "        # outs = []    # save all predictions\n",
    "        # for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "        #     outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        # return torch.stack(outs, dim=1), h_state\n",
    "\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "\n",
    "def train_reg(data, Z, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    # X_lagged = np.concatenate((np.ones(shape=(1,1), dtype=np.float)*data[0,0], np.expand_dims((data[0:-1:, 0]), 0)), 1)\n",
    "    # Y = np.concatenate((X_lagged.T, data[:, 1:]), 1)\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "        Z_ = Z[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(torch.cat((Y_, Z_), axis = 1), h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(torch.cat((valid_Y, valid_Z), axis = 1), h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def test(data, Z, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "    \n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(torch.cat((Y, Z), axis = 1), h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_optim = optim.Adam(regr_net.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred = train_reg(train_data, train_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred = train_reg(valid_data, valid_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "\n",
    "    regr_trained = regr_net\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred = test(test_data, test_Z, regr_net)\n",
    "    MSEloss = np.mean(np.square(test_data[10:,0] - X_pred[10:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_586517/4262776442.py:39: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "  0%|          | 3/50000 [00:00<28:47, 28.94it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [22:55<00:00, 36.36it/s]\n",
      "100%|██████████| 50000/50000 [22:58<00:00, 36.27it/s]\n",
      "100%|██████████| 50000/50000 [23:02<00:00, 36.17it/s]\n",
      "100%|██████████| 50000/50000 [23:01<00:00, 36.20it/s]\n",
      "100%|██████████| 50000/50000 [22:55<00:00, 36.34it/s]\n",
      "100%|██████████| 50000/50000 [23:03<00:00, 36.13it/s]\n",
      "100%|██████████| 50000/50000 [22:58<00:00, 36.26it/s]\n",
      "100%|██████████| 50000/50000 [22:57<00:00, 36.31it/s]\n",
      "100%|██████████| 50000/50000 [23:03<00:00, 36.14it/s]\n",
      "100%|██████████| 50000/50000 [23:03<00:00, 36.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.07195284975185e-05\n"
     ]
    }
   ],
   "source": [
    "def test_without_Z(data, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(Y, h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "def train_reg_withoutZ(data, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(Y_, h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(valid_Y, h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z_optim = optim.Adam(regr_net_without_Z.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred_without_Z = train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred_without_Z = train_reg_withoutZ(valid_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "\n",
    "    regr_without_Z_trained = regr_net_without_Z\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred_without_Z = test_without_Z(test_data, regr_net_without_Z)\n",
    "    MSEloss = np.mean(np.square(test_data[:,0] - X_pred_without_Z[:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_586517/3294305005.py:55: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
      "  0%|          | 3/50000 [00:00<33:07, 25.16it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [31:34<00:00, 26.39it/s]\n",
      "100%|██████████| 50000/50000 [31:18<00:00, 26.62it/s]\n",
      "100%|██████████| 50000/50000 [31:28<00:00, 26.47it/s]\n",
      "100%|██████████| 50000/50000 [31:30<00:00, 26.45it/s]\n",
      "100%|██████████| 50000/50000 [31:29<00:00, 26.46it/s]\n",
      "100%|██████████| 50000/50000 [31:22<00:00, 26.56it/s]\n",
      "100%|██████████| 50000/50000 [31:24<00:00, 26.53it/s]\n",
      "100%|██████████| 50000/50000 [31:28<00:00, 26.48it/s]\n",
      "100%|██████████| 50000/50000 [31:31<00:00, 26.44it/s]\n",
      "100%|██████████| 50000/50000 [32:50<00:00, 25.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7681610767417165e-05\n"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden_size = 32, num_layers = 1, output_size = 2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,     # rnn hidden unit\n",
    "            num_layers = num_layers,       # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        # print(r_out.shape)\n",
    "        # outs = []    # save all predictions\n",
    "        # for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "        #     outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        # return torch.stack(outs, dim=1), h_state\n",
    "\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "\n",
    "def train_reg(data, Z, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    # X_lagged = np.concatenate((np.ones(shape=(1,1), dtype=np.float)*data[0,0], np.expand_dims((data[0:-1:, 0]), 0)), 1)\n",
    "    # Y = np.concatenate((X_lagged.T, data[:, 1:]), 1)\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "        Z_ = Z[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(torch.cat((Y_, Z_), axis = 1), h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(torch.cat((valid_Y, valid_Z), axis = 1), h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def test(data, Z, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "    \n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(torch.cat((Y, Z), axis = 1), h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_optim = optim.Adam(regr_net.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred = train_reg(train_data, train_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred = train_reg(valid_data, valid_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "\n",
    "    regr_trained = regr_net\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred = test(test_data, test_Z, regr_net)\n",
    "    MSEloss = np.mean(np.square(test_data[10:,0] - X_pred[10:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]/tmp/ipykernel_586517/1802000487.py:39: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50000/50000 [33:48<00:00, 24.64it/s]\n",
      "100%|██████████| 50000/50000 [33:57<00:00, 24.54it/s]\n",
      "100%|██████████| 50000/50000 [33:52<00:00, 24.60it/s]\n",
      "100%|██████████| 50000/50000 [33:53<00:00, 24.59it/s]\n",
      "100%|██████████| 50000/50000 [33:53<00:00, 24.59it/s]\n",
      "100%|██████████| 50000/50000 [33:43<00:00, 24.71it/s]\n",
      "100%|██████████| 50000/50000 [33:44<00:00, 24.70it/s]\n",
      "100%|██████████| 50000/50000 [33:52<00:00, 24.60it/s]\n",
      "100%|██████████| 50000/50000 [33:52<00:00, 24.60it/s]\n",
      "100%|██████████| 50000/50000 [28:22<00:00, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.458490606526428e-05\n"
     ]
    }
   ],
   "source": [
    "def test_without_Z(data, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(Y, h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "def train_reg_withoutZ(data, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(Y_, h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(valid_Y, h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z_optim = optim.Adam(regr_net_without_Z.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred_without_Z = train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred_without_Z = train_reg_withoutZ(valid_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "\n",
    "    regr_without_Z_trained = regr_net_without_Z\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred_without_Z = test_without_Z(test_data, regr_net_without_Z)\n",
    "    MSEloss = np.mean(np.square(test_data[:,0] - X_pred_without_Z[:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "exchangedata = pd.read_csv('dataset/exchange_rate/exchange_rate.csv')\n",
    "exchangedata = exchangedata.drop('date', axis=1)\n",
    "target = exchangedata.pop('OT')\n",
    "exchangedata.insert(0, 'OT', target)\n",
    "data1 = exchangedata\n",
    "pcmci_result = np.array([1, -1, 3, 5, 3, -1, 1, 1]).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data = data1.iloc[:, pcmci_result > -1]\n",
    "sorted_arrows = pcmci_result[pcmci_result > -1].astype(int)\n",
    "\n",
    "tau_max = 20\n",
    "ar_range = 10\n",
    "\n",
    "lagged_data = pd.DataFrame(sorted_data.iloc[tau_max:, 0])\n",
    "for i in range(1, ar_range):\n",
    "    lagged_data['Target' + '_' + str(i)] = np.array(sorted_data.iloc[tau_max-i:-i, 0])\n",
    "for i in range(sorted_data.shape[1]):\n",
    "    if sorted_arrows[i] == 0:\n",
    "        lagged_data[sorted_data.columns[i] + '_' + str(sorted_arrows[i])] = np.array(sorted_data.iloc[tau_max:, i])\n",
    "        continue\n",
    "    lagged_data[sorted_data.columns[i] + '_' + str(sorted_arrows[i])] = np.array(sorted_data.iloc[tau_max - sorted_arrows[i]:-sorted_arrows[i], i])\n",
    "\n",
    "minmaxgap =  lagged_data.iloc[:, 0].max() - lagged_data.iloc[:, 0].min()\n",
    "lagged_data = (lagged_data - lagged_data.min()) / ( lagged_data.max() - lagged_data.min())\n",
    "\n",
    "data2 = lagged_data.copy()\n",
    "data = np.array(data2)\n",
    "data.shape\n",
    "\n",
    "train_proportion = 0.6\n",
    "valid_proportion = 0.2\n",
    "test_proportion = 0.2\n",
    "\n",
    "train_size = int(data.shape[0] * train_proportion)\n",
    "valid_size = int(data.shape[0] * valid_proportion)\n",
    "\n",
    "train_data = data[:train_size]\n",
    "valid_data = data[train_size:train_size+valid_size]\n",
    "test_data = data[train_size+valid_size:]\n",
    "\n",
    "batch_size = 20\n",
    "window_size = 50\n",
    "\n",
    "z_size = data.shape[1]\n",
    "\n",
    "\n",
    "Z = np.load('results/exchange_rate_z.npy')\n",
    "\n",
    "train_Z = Z[:train_size]\n",
    "valid_Z = Z[train_size:train_size+valid_size]\n",
    "test_Z = Z[train_size+valid_size:]\n",
    "\n",
    "###\n",
    "valid_Z = torch.autograd.Variable(torch.FloatTensor(valid_Z)).cuda()\n",
    "###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4540"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1513, 15])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 31, got 30",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/dell/桌面/TCDF-copy/Recurrenttoy0+RegX+RNN.ipynb Cell 35\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m regr_net \u001b[39m=\u001b[39m RNN(input_size \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\u001b[39m+\u001b[39mz_size, hidden_size \u001b[39m=\u001b[39m \u001b[39m256\u001b[39m, num_layers \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m, output_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m regr_net_optim \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(regr_net\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m, weight_decay \u001b[39m=\u001b[39m \u001b[39m1e-5\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m loss_array, train_X_pred \u001b[39m=\u001b[39m train_reg(train_data, train_Z, regr_net, regr_net_optim, iter_num \u001b[39m=\u001b[39;49m \u001b[39m50000\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39m# loss_array, train_X_pred = train_reg(valid_data, valid_Z, regr_net, regr_net_optim, iter_num = 50000)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m regr_trained \u001b[39m=\u001b[39m regr_net\n",
      "\u001b[1;32m/home/dell/桌面/TCDF-copy/Recurrenttoy0+RegX+RNN.ipynb Cell 35\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m Z_ \u001b[39m=\u001b[39m Z[pivot:pivot\u001b[39m+\u001b[39mwindow_size]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m h_state \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m X_pred, h_state \u001b[39m=\u001b[39m regr_net(torch\u001b[39m.\u001b[39;49mcat((Y_, Z_), axis \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m), h_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m mseloss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mMSELoss(reduction \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m loss \u001b[39m=\u001b[39m mseloss(X_pred, X_)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/home/dell/桌面/TCDF-copy/Recurrenttoy0+RegX+RNN.ipynb Cell 35\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, h_state):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     r_out, h_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrnn(x, h_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# print(r_out.shape)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# outs = []    # save all predictions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     \u001b[39m# for time_step in range(r_out.size(1)):    # calculate output for each time step\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m#     outs.append(self.out(r_out[:, time_step, :]))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# return torch.stack(outs, dim=1), h_state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dell/%E6%A1%8C%E9%9D%A2/TCDF-copy/Recurrenttoy0%2BRegX%2BRNN.ipynb#X46sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     outs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout(r_out)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:472\u001b[0m, in \u001b[0;36mRNN.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    469\u001b[0m     hx \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    471\u001b[0m \u001b[39massert\u001b[39;00m hx \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_forward_args(\u001b[39minput\u001b[39;49m, hx, batch_sizes)\n\u001b[1;32m    473\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_TANH\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mRNN_RELU\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    474\u001b[0m \u001b[39mif\u001b[39;00m batch_sizes \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:234\u001b[0m, in \u001b[0;36mRNNBase.check_forward_args\u001b[0;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck_forward_args\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):\n\u001b[0;32m--> 234\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheck_input(\u001b[39minput\u001b[39;49m, batch_sizes)\n\u001b[1;32m    235\u001b[0m     expected_hidden_size \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_expected_hidden_size(\u001b[39minput\u001b[39m, batch_sizes)\n\u001b[1;32m    237\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_hidden_size(hidden, expected_hidden_size)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/rnn.py:210\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[0;34m(self, input, batch_sizes)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    207\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput must have \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m dimensions, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    208\u001b[0m             expected_input_dim, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdim()))\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size \u001b[39m!=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m--> 210\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    211\u001b[0m         \u001b[39m'\u001b[39m\u001b[39minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    212\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_size, \u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 31, got 30"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden_size = 32, num_layers = 1, output_size = 2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,     # rnn hidden unit\n",
    "            num_layers = num_layers,       # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        # print(r_out.shape)\n",
    "        # outs = []    # save all predictions\n",
    "        # for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "        #     outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        # return torch.stack(outs, dim=1), h_state\n",
    "\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "\n",
    "def train_reg(data, Z, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    # X_lagged = np.concatenate((np.ones(shape=(1,1), dtype=np.float)*data[0,0], np.expand_dims((data[0:-1:, 0]), 0)), 1)\n",
    "    # Y = np.concatenate((X_lagged.T, data[:, 1:]), 1)\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "        Z_ = Z[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(torch.cat((Y_, Z_), axis = 1), h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(torch.cat((valid_Y, valid_Z), axis = 1), h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def test(data, Z, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "    \n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(torch.cat((Y, Z), axis = 1), h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = 256, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_optim = optim.Adam(regr_net.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred = train_reg(train_data, train_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred = train_reg(valid_data, valid_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "\n",
    "    regr_trained = regr_net\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = 256, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred = test(test_data, test_Z, regr_net)\n",
    "    MSEloss = np.mean(np.square(test_data[10:,0] - X_pred[10:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m min_loss \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m---> 56\u001b[0m     regr_net_without_Z \u001b[39m=\u001b[39m RNN(input_size \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, hidden_size \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m, num_layers \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m, output_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     57\u001b[0m     regr_net_without_Z_optim \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(regr_net_without_Z\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m, weight_decay \u001b[39m=\u001b[39m \u001b[39m1e-5\u001b[39m)\n\u001b[1;32m     58\u001b[0m     loss_array, train_X_pred_without_Z \u001b[39m=\u001b[39m train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num \u001b[39m=\u001b[39m \u001b[39m50000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RNN' is not defined"
     ]
    }
   ],
   "source": [
    "def test_without_Z(data, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(Y, h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "def train_reg_withoutZ(data, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(Y_, h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(valid_Y, h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z_optim = optim.Adam(regr_net_without_Z.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred_without_Z = train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred_without_Z = train_reg_withoutZ(valid_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "\n",
    "    regr_without_Z_trained = regr_net_without_Z\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred_without_Z = test_without_Z(test_data, regr_net_without_Z)\n",
    "    MSEloss = np.mean(np.square(test_data[:,0] - X_pred_without_Z[:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mRNN\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[1;32m      2\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, input_size \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m, hidden_size \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m, num_layers \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m, output_size \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m      3\u001b[0m         \u001b[39msuper\u001b[39m(RNN, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size = 2, hidden_size = 32, num_layers = 1, output_size = 2):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = input_size,\n",
    "            hidden_size = hidden_size,     # rnn hidden unit\n",
    "            num_layers = num_layers,       # number of rnn layer\n",
    "            batch_first = True,   # input & output will has batch size as 1s dimension. e.g. (batch, time_step, input_size)\n",
    "        )\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h_state):\n",
    "        r_out, h_state = self.rnn(x, h_state)\n",
    "        # print(r_out.shape)\n",
    "        # outs = []    # save all predictions\n",
    "        # for time_step in range(r_out.size(1)):    # calculate output for each time step\n",
    "        #     outs.append(self.out(r_out[:, time_step, :]))\n",
    "\n",
    "        # return torch.stack(outs, dim=1), h_state\n",
    "\n",
    "        outs = self.out(r_out)\n",
    "        return outs, h_state\n",
    "\n",
    "def train_reg(data, Z, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    # X_lagged = np.concatenate((np.ones(shape=(1,1), dtype=np.float)*data[0,0], np.expand_dims((data[0:-1:, 0]), 0)), 1)\n",
    "    # Y = np.concatenate((X_lagged.T, data[:, 1:]), 1)\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "        Z_ = Z[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(torch.cat((Y_, Z_), axis = 1), h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(torch.cat((valid_Y, valid_Z), axis = 1), h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "def test(data, Z, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "    Z = torch.autograd.Variable(torch.FloatTensor(Z)).cuda()\n",
    "    \n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(torch.cat((Y, Z), axis = 1), h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_optim = optim.Adam(regr_net.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred = train_reg(train_data, train_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred = train_reg(valid_data, valid_Z, regr_net, regr_net_optim, iter_num = 50000)\n",
    "\n",
    "    regr_trained = regr_net\n",
    "    regr_net = RNN(input_size = data.shape[1]-1+z_size, hidden_size = z_size * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred = test(test_data, test_Z, regr_net)\n",
    "    MSEloss = np.mean(np.square(test_data[10:,0] - X_pred[10:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'RNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 57\u001b[0m\n\u001b[1;32m     55\u001b[0m min_loss \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[0;32m---> 57\u001b[0m     regr_net_without_Z \u001b[39m=\u001b[39m RNN(input_size \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, hidden_size \u001b[39m=\u001b[39m data\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39m*\u001b[39m \u001b[39m4\u001b[39m, num_layers \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m, output_size \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m     58\u001b[0m     regr_net_without_Z_optim \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mAdam(regr_net_without_Z\u001b[39m.\u001b[39mparameters(), lr \u001b[39m=\u001b[39m \u001b[39m1e-3\u001b[39m, weight_decay \u001b[39m=\u001b[39m \u001b[39m1e-5\u001b[39m)\n\u001b[1;32m     59\u001b[0m     loss_array, train_X_pred_without_Z \u001b[39m=\u001b[39m train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num \u001b[39m=\u001b[39m \u001b[39m50000\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'RNN' is not defined"
     ]
    }
   ],
   "source": [
    "def test_without_Z(data, regr_net):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    h_state = None\n",
    "    X_pred, h_state = regr_net(Y, h_state)\n",
    "    mseloss = torch.nn.MSELoss(reduction = 'sum')\n",
    "    loss = mseloss(X_pred, X)\n",
    "    return loss.detach(), X_pred.detach().cpu().numpy()\n",
    "\n",
    "def train_reg_withoutZ(data, regr_net, regr_net_optim, iter_num = 5000):\n",
    "    X = np.expand_dims(data[:, 0], 1)\n",
    "    Y = data[:, 1:]\n",
    "    X = torch.autograd.Variable(torch.FloatTensor(X)).cuda()\n",
    "    Y = torch.autograd.Variable(torch.FloatTensor(Y)).cuda()\n",
    "\n",
    "    ###\n",
    "    valid_X = np.expand_dims(valid_data[:, 0], 1)\n",
    "    valid_Y = valid_data[:, 1:]\n",
    "    valid_X = torch.autograd.Variable(torch.FloatTensor(valid_X)).cuda()\n",
    "    valid_Y = torch.autograd.Variable(torch.FloatTensor(valid_Y)).cuda()\n",
    "    ###\n",
    "\n",
    "    best_result = 1000\n",
    "    loss_array = []\n",
    "    for _ in tqdm(range(iter_num)):\n",
    "        pivot = np.random.randint(X.shape[0]-window_size)\n",
    "        X_ = X[pivot:pivot+window_size]\n",
    "        Y_ = Y[pivot:pivot+window_size]\n",
    "\n",
    "        h_state = None\n",
    "        X_pred, h_state = regr_net(Y_, h_state)\n",
    "        mseloss = torch.nn.MSELoss(reduction = 'mean')\n",
    "        loss = mseloss(X_pred, X_)\n",
    "        regr_net_optim.zero_grad()\n",
    "        autograd.backward(loss)\n",
    "        nn.utils.clip_grad_norm(regr_net.parameters(), max_norm=0.001, norm_type=2)\n",
    "        regr_net_optim.step()\n",
    "        loss_array.append(loss.detach().cpu().numpy())\n",
    "\n",
    "        ###\n",
    "        h_state = None\n",
    "        X_valid_pred, h_state = regr_net(valid_Y, h_state)\n",
    "        loss = mseloss(valid_X, X_valid_pred).detach().cpu().numpy()\n",
    "        ###\n",
    "\n",
    "        if loss < best_result:\n",
    "            best_result = loss\n",
    "            torch.save(regr_net.state_dict(), 'regr_net.pt')\n",
    "    \n",
    "    return loss_array, X_pred.detach().cpu().numpy()\n",
    "\n",
    "min_loss = 1000\n",
    "for i in range(10):\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z_optim = optim.Adam(regr_net_without_Z.parameters(), lr = 1e-3, weight_decay = 1e-5)\n",
    "    loss_array, train_X_pred_without_Z = train_reg_withoutZ(train_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "    # loss_array, train_X_pred_without_Z = train_reg_withoutZ(valid_data, regr_net_without_Z, regr_net_without_Z_optim, iter_num = 50000)\n",
    "\n",
    "    regr_without_Z_trained = regr_net_without_Z\n",
    "    regr_net_without_Z = RNN(input_size = data.shape[1]-1, hidden_size = data.shape[1] * 4, num_layers = 2, output_size = 1).cuda()\n",
    "    regr_net_without_Z.load_state_dict(torch.load('regr_net.pt'))\n",
    "\n",
    "    test_loss, X_pred_without_Z = test_without_Z(test_data, regr_net_without_Z)\n",
    "    MSEloss = np.mean(np.square(test_data[:,0] - X_pred_without_Z[:].T))# * minmaxgap\n",
    "    if MSEloss < min_loss:\n",
    "        min_loss = MSEloss\n",
    "\n",
    "print(min_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
